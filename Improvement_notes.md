# üöÄ Crypto Market Intelligence Pipeline - Improvement notes

## üìä Current vs. Enhanced Comparison

| Feature | Before | After |
|---------|--------|-------|
| Error Handling | ‚ùå None | ‚úÖ Comprehensive try-catch |
| API Retry Logic | ‚ùå None | ‚úÖ Exponential backoff |
| Data Validation | ‚ùå None | ‚úÖ Multi-level validation |
| Model Evaluation | ‚ùå None | ‚úÖ MAE, RMSE, MAPE, Direction |
| API Model Loading | ‚ùå Every request | ‚úÖ Cached on startup |
| Logging | ‚ùå None | ‚úÖ File + Console |
| Requirements Versions | ‚ùå Unpinned | ‚úÖ Version locked |
| Tests | ‚ùå None | ‚úÖ Full test suite |
| CI/CD Robustness | ‚ö†Ô∏è Basic | ‚úÖ 7-job pipeline |
| Model Versioning | ‚ùå None | ‚úÖ Metadata tracking |
| API Endpoints | ‚ö†Ô∏è 1 endpoint | ‚úÖ 8+ endpoints |
| Docker | ‚ö†Ô∏è Basic | ‚úÖ Multi-stage + health checks |
| Documentation | ‚ö†Ô∏è Minimal | ‚úÖ Comprehensive |

---

## üéØ Implementation Plan

### **Phase 1: Critical Fixes **

#### 1. Replace All Python Files
Copy the enhanced versions I provided:
- ‚úÖ `data_ingestion.py` - Retry logic, validation, multi-coin support
- ‚úÖ `data_processing.py` - Feature engineering, stationarity checks
- ‚úÖ `model_training.py` - Auto-tuning, evaluation, metadata
- ‚úÖ `api_service.py` - Model caching, 8 endpoints, error handling
- ‚úÖ `requirements.txt` - Version pinned dependencies
- ‚úÖ `Dockerfile` - Multi-stage build, health checks
- ‚úÖ `.github/workflows/ci_cd.yml` - Robust 7-job pipeline

#### 2. Create New Files

**Create `tests/` directory:**
```bash
mkdir tests
touch tests/__init__.py
```
Copy `tests/test_pipeline.py` from the artifact I created.

**Create `.env` file:**
```bash
# .env
LOG_LEVEL=INFO
WORKERS=2
```

**Create `docker-compose.yml`:**
Copy from the artifact for local development.

#### 3. Update Your Workflow

**Before running CI/CD, set up GitHub Secrets (optional):**
- Go to Settings ‚Üí Secrets ‚Üí Actions
- Add `DOCKER_USERNAME` and `DOCKER_PASSWORD` if deploying to Docker Hub

---

### **Phase 2: Testing & Validation (Day 3)**

#### 1. Run Tests Locally
```bash
# Install test dependencies
pip install pytest pytest-cov

# Run tests
pytest tests/ -v --cov

# Expected output: Most tests should pass
# Some may skip if API is rate-limited
```

#### 2. Test Data Pipeline
```bash
# Test ingestion
python data_ingestion.py --days 7

# Expected: crypto_prices.csv created with ~168 records

# Test processing
python data_processing.py

# Expected: processed_prices.csv created

# Test training
python model_training.py --auto-order

# Expected: arima_model.pkl and model_metadata.json created
```

#### 3. Test API Locally
```bash
# Start API
uvicorn api_service:app --reload

# In another terminal, test endpoints:
curl http://localhost:8000/health
curl http://localhost:8000/predict
curl http://localhost:8000/model/info
curl http://localhost:8000/data/latest
```

---

### **Phase 3: Advanced Features ( 2)**

#### 4. Add Model Auto-Retraining Schedule

**Create `retrain_scheduler.py`:**
```python
import schedule
import time
import subprocess
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def retrain_model():
    """Run full pipeline to retrain model"""
    logger.info("Starting model retraining...")
    
    try:
        # Run data ingestion
        subprocess.run(["python", "data_ingestion.py"], check=True)
        
        # Run processing
        subprocess.run(["python", "data_processing.py"], check=True)
        
        # Train model
        subprocess.run(["python", "model_training.py", "--auto-order"], check=True)
        
        logger.info("‚úì Model retraining completed")
        
    except Exception as e:
        logger.error(f"‚úó Retraining failed: {e}")

# Schedule retraining daily at 2 AM
schedule.every().day.at("02:00").do(retrain_model)

if __name__ == "__main__":
    logger.info("Scheduler started")
    while True:
        schedule.run_pending()
        time.sleep(60)
```

#### 5. Add Monitoring with Prometheus

**Create `prometheus.yml`:**
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'crypto-api'
    static_configs:
      - targets: ['api:8000']
```

**Add metrics to API** (`api_service.py`):
```python
from prometheus_client import Counter, Histogram, make_asgi_app

# Metrics
prediction_counter = Counter('predictions_total', 'Total predictions')
prediction_latency = Histogram('prediction_duration_seconds', 'Prediction latency')

@app.get("/predict")
@prediction_latency.time()
async def get_prediction(steps: int = 1):
    prediction_counter.inc()
    # ... rest of code

# Mount metrics endpoint
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)
```

#### 6. Add Multiple Models

**Enhance `model_training.py` to support Prophet:**
```python
from prophet import Prophet

def train_prophet_model(df: pd.DataFrame) -> Prophet:
    """Train Prophet model"""
    # Prophet requires 'ds' and 'y' columns
    prophet_df = df.reset_index()
    prophet_df.columns = ['ds', 'y']
    
    model = Prophet(
        changepoint_prior_scale=0.05,
        seasonality_prior_scale=10
    )
    model.fit(prophet_df)
    
    return model
```

#### 7. Add Model Comparison Dashboard

**Create `compare_models.py`:**
```python
import pandas as pd
import json
from model_training import train_arima_model, evaluate_model
from prophet import Prophet

def compare_models(data_file: str = "processed_prices.csv"):
    """Compare ARIMA vs Prophet"""
    df = pd.read_csv(data_file, index_col="timestamp", parse_dates=True)
    
    # Split data
    train = df["price"][:-20]
    test = df["price"][-20:]
    
    # Train ARIMA
    arima = train_arima_model(train, order=(5, 1, 0))
    arima_metrics = evaluate_model(arima, test)
    
    # Train Prophet
    prophet_df = train.reset_index()
    prophet_df.columns = ['ds', 'y']
    prophet = Prophet()
    prophet.fit(prophet_df)
    
    # Compare
    comparison = {
        "arima": arima_metrics,
        "prophet": {},  # Add Prophet metrics
        "winner": "arima" if arima_metrics["rmse"] < 1000 else "prophet"
    }
    
    print(json.dumps(comparison, indent=2))
    return comparison
```

---

### **Phase 4: Production Deployment ( 3)**

#### 8. Deploy to Cloud

**Option A: Deploy to Heroku**
```bash
# Install Heroku CLI
# heroku login

# Create app
heroku create crypto-prediction-api

# Add Procfile
echo "web: uvicorn api_service:app --host=0.0.0.0 --port=\$PORT" > Procfile

# Deploy
git push heroku main

# Scale
heroku ps:scale web=1
```

**Option B: Deploy to Google Cloud Run**
```bash
# Build and push Docker image
gcloud builds submit --tag gcr.io/PROJECT_ID/crypto-api

# Deploy
gcloud run deploy crypto-api \
  --image gcr.io/PROJECT_ID/crypto-api \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

**Option C: Deploy to AWS ECS**
```bash
# Create ECR repository
aws ecr create-repository --repository-name crypto-api

# Build and push
docker build -t crypto-api .
docker tag crypto-api:latest AWS_ACCOUNT.dkr.ecr.REGION.amazonaws.com/crypto-api:latest
docker push AWS_ACCOUNT.dkr.ecr.REGION.amazonaws.com/crypto-api:latest

# Create ECS task definition and service
# (Use AWS Console or CloudFormation)
```

#### 9. Set Up Domain and SSL

**Using Cloudflare (free):**
1. Point domain to deployment
2. Enable SSL/TLS encryption
3. Add rate limiting rules
4. Enable caching for `/model/info`

#### 10. Add Alerting

**Create `alerting.py`:**
```python
import requests
from datetime import datetime

SLACK_WEBHOOK = "https://hooks.slack.com/services/YOUR/WEBHOOK"

def send_alert(message: str, level: str = "info"):
    """Send alert to Slack"""
    emoji = {"info": "‚ÑπÔ∏è", "warning": "‚ö†Ô∏è", "error": "üî•"}
    
    payload = {
        "text": f"{emoji.get(level, '‚ÑπÔ∏è')} *{level.upper()}*\n{message}",
        "username": "Crypto Pipeline Bot"
    }
    
    requests.post(SLACK_WEBHOOK, json=payload)

# Usage in model_training.py
if metrics["rmse"] > 1000:
    send_alert(f"High RMSE detected: {metrics['rmse']:.2f}", level="warning")
```

---

## üìà Advanced Optimizations

### Performance Improvements

#### 1. Add Caching Layer
```python
from functools import lru_cache
import redis

redis_client = redis.Redis(host='localhost', port=6379)

@lru_cache(maxsize=100)
def get_cached_prediction(steps: int):
    """Cache predictions for 5 minutes"""
    # Implementation
```

#### 2. Use Async Data Loading
```python
import asyncio
import aiohttp

async def fetch_multiple_coins_async(coins: list):
    """Fetch multiple coins in parallel"""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_coin_async(session, coin) for coin in coins]
        results = await asyncio.gather(*tasks)
    return results
```

#### 3. Model Quantization
```python
# Reduce model size for faster loading
import pickle
import gzip

# Save compressed
with gzip.open('arima_model.pkl.gz', 'wb') as f:
    pickle.dump(model, f)

# Load compressed
with gzip.open('arima_model.pkl.gz', 'rb') as f:
    model = pickle.load(f)
```

---

## üîê Security Enhancements

### 1. Add API Key Authentication
```python
from fastapi import Security, HTTPException
from fastapi.security import APIKeyHeader

API_KEY = "your-secret-key"
api_key_header = APIKeyHeader(name="X-API-Key")

def verify_api_key(api_key: str = Security(api_key_header)):
    if api_key != API_KEY:
        raise HTTPException(status_code=403, detail="Invalid API Key")
    return api_key

@app.get("/predict")
async def get_prediction(api_key: str = Security(verify_api_key)):
    # Protected endpoint
```

### 2. Rate Limiting
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.get("/predict")
@limiter.limit("10/minute")
async def get_prediction(request: Request):
    # Rate limited endpoint
```

### 3. Input Validation
```python
from pydantic import BaseModel, validator

class PredictionRequest(BaseModel):
    steps: int
    
    @validator('steps')
    def validate_steps(cls, v):
        if v < 1 or v > 100:
            raise ValueError('Steps must be between 1 and 100')
        return v
```

---

## üìä Monitoring & Observability

### Create Dashboard in Grafana

**Import Dashboard JSON:**
```json
{
  "dashboard": {
    "title": "Crypto API Metrics",
    "panels": [
      {
        "title": "Prediction Requests",
        "targets": [{
          "expr": "rate(predictions_total[5m])"
        }]
      },
      {
        "title": "API Latency",
        "targets": [{
          "expr": "histogram_quantile(0.95, prediction_duration_seconds)"
        }]
      }
    ]
  }
}
```

---

## ‚úÖ Final Checklist

### Before Production
- [ ] All tests passing (`pytest tests/`)
- [ ] CI/CD pipeline successful
- [ ] Docker image builds correctly
- [ ] Health checks working
- [ ] Monitoring configured
- [ ] Alerts set up
- [ ] Documentation updated
- [ ] Security review completed
- [ ] Performance tested (load testing)
- [ ] Backup strategy defined
- [ ] Rollback plan documented

### Post-Deployment
- [ ] Monitor error rates
- [ ] Check prediction accuracy
- [ ] Review API latency
- [ ] Monitor resource usage
- [ ] Set up scheduled retraining
- [ ] Document any issues
- [ ] Gather user feedback

---

## üéì Learning Resources

- **FastAPI**: https://fastapi.tiangolo.com/
- **ARIMA**: https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html
- **Time Series Forecasting**: https://otexts.com/fpp3/
- **Docker Best Practices**: https://docs.docker.com/develop/dev-best-practices/
- **GitHub Actions**: https://docs.github.com/en/actions
- **API Design**: https://www.restapitutorial.com/

---

## üÜò Troubleshooting

### Common Issues

**Issue**: "Model file not found"
**Solution**: Run `python model_training.py` first

**Issue**: "API returns 503"
**Solution**: Check if model loaded successfully, verify file paths

**Issue**: "High prediction error"
**Solution**: Retrain with more data, try SARIMA for seasonality

**Issue**: "CI/CD fails on data ingestion"
**Solution**: CoinGecko may be rate limiting, add delays

**Issue**: "Docker build fails"
**Solution**: Check Dockerfile syntax, ensure all files exist

---

## üìû Quick Commands Reference

```bash
# Local Development
python data_ingestion.py --days 30
python data_processing.py
python model_training.py --auto-order
uvicorn api_service:app --reload

# Testing
pytest tests/ -v
black *.py
flake8 *.py

# Docker
docker build -t crypto-api .
docker run -p 8000:8000 crypto-api
docker-compose up -d

# Git
git add .
git commit -m "Enhanced pipeline"
git push origin main
```

